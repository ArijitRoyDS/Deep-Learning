{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Vanishing Gradient Problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanishing gradient problem is a challenge that arises during the training of deep neural networks, particularly those with many layers. It refers to the issue where the gradients of the loss function become extremely small as they are backpropagated from the later layers to the earlier layers of the network. Consequently, the weights in the early layers receive minimal updates, impeding effective learning.\n",
    "\n",
    "When backpropagating gradients, the algorithm multiplies the gradients at each layer with the weight matrices as it moves backward through the network. This multiplication can result in the gradients becoming exponentially smaller as they propagate backward, especially when using activation functions with limited ranges such as the sigmoid function.\n",
    "\n",
    "As a consequence of the vanishing gradients, the early layers of the network may fail to learn meaningful representations or fail to update their weights significantly during training. This issue is particularly prominent in deep networks with many layers, making it difficult to train them effectively.\n",
    "\n",
    "The vanishing gradient problem was recognized as one of the major obstacles in training deep neural networks until the development of new techniques that helped mitigate its impact. Some approaches to addressing the vanishing gradient problem include:\n",
    "\n",
    "1. Activation functions: Replacing sigmoid or tanh activation functions with alternatives that have a larger range, such as ReLU (Rectified Linear Unit), can alleviate the vanishing gradient problem. ReLU and its variants allow gradients to flow more freely and reduce the likelihood of gradients vanishing.\n",
    "\n",
    "2. Weight initialization: Appropriate weight initialization methods, such as the Glorot or He initialization, can help alleviate the vanishing gradient problem by ensuring that the initial weights are set in a range that avoids saturation of the activation function.\n",
    "\n",
    "3. Batch normalization: Batch normalization normalizes the inputs of each layer, which helps with the vanishing gradient problem by reducing the covariance shift. It can stabilize the network's learning process and make it easier to train deep networks.\n",
    "\n",
    "4. Skip connections: Introducing skip connections, such as in residual networks (ResNet), allows gradients to bypass some layers and flow more directly, mitigating the vanishing gradient problem and enabling the training of very deep networks.\n",
    "\n",
    "These techniques, among others, have been successful in overcoming the vanishing gradient problem and have contributed to the effective training of deep neural networks with numerous layers. However, it is important to note that the vanishing gradient problem is just one of several challenges encountered in deep learning, and different techniques may be required depending on the specific problem and architecture being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_Rc2sY4gxQqO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.datasets import make_moons # inbuild dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense # Fully connected\n",
    "from keras.models import Sequential # architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1vRkSiUzxcd4"
   },
   "outputs": [],
   "source": [
    "# Create a dataset with 250 samples and 5% noise (out of range random values)\n",
    "X, y = make_moons(n_samples = 250, noise=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "G4Fmlkeoxcg6",
    "outputId": "daa0e7b6-9724-4025-d151-33e955bd67ef"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c = y, s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tMKKALZWzQhA"
   },
   "outputs": [],
   "source": [
    "# First test the accuracy with multiple hidden layers\n",
    "\n",
    "model = Sequential()           # Sequential = Each neuron of one layer connected with each neuron of next layer\n",
    "model.add(Dense(10, activation = 'sigmoid', input_dim=2))   # Dense Hidden Layer means fully connected. 10 neurons per layer\n",
    "model.add(Dense(10, activation = 'sigmoid'))                # Adding multiple hidden layers\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(1 , activation = 'sigmoid'))                # At final output I need only 1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sMO2LusWzQkE"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KA9XRJXEzQmp",
    "outputId": "e105b7ba-332d-4ae7-da5a-895db899ccb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45690113, -0.35882208, -0.21778253,  0.30903298,  0.38628584,\n",
       "        -0.2837154 , -0.15365845, -0.31729227, -0.12548214,  0.04472136],\n",
       "       [ 0.3881424 , -0.04924858,  0.10061955, -0.01443797,  0.04960918,\n",
       "         0.15672725, -0.44318455, -0.40585256, -0.18910766, -0.57734436]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[0]\n",
    "# We get 20 weights because each of the 2 inputs connect to 10 neurons of the 1st hidden layer (10 * 2 = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pnloreMvzQpi"
   },
   "outputs": [],
   "source": [
    "# Store the above weights for later reference:\n",
    "old_weights = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsAjUjGkzQvT",
    "outputId": "07ccbe06-7334-44d3-bd08-392600ac32d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 1s 2ms/step - loss: 0.6997 - accuracy: 0.4900\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6969 - accuracy: 0.4900\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6956 - accuracy: 0.4900\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6946 - accuracy: 0.4900\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6939 - accuracy: 0.4900\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4900\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4900\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5100\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5100\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5100\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5100\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5100\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5100\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5100\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5100\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5100\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5100\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4600\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.4900\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5100\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5100\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5100\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.4900\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.4900\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4900\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4900\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.4900\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4800\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6935 - accuracy: 0.5100\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4800\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4900\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4900\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5100\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5100\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5100\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5100\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5100\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5100\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5100\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5100\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5100\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5100\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5100\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6935 - accuracy: 0.5100\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6936 - accuracy: 0.5100\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.5100\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5100\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5100\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a589a8eb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100)\n",
    "# Note that after 8th epoch, loss value becomes almost constant which results in Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Hu4u9Q1Txcka"
   },
   "outputs": [],
   "source": [
    "# Get the new weights after building the model\n",
    "new_weights = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the Old and New weights. There is very minute difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "voYK5UY63FpX",
    "outputId": "b96ee30d-35ea-43d8-dcbd-61445e818daa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45895168, -0.3566534 , -0.22506502,  0.31524876,  0.38640818,\n",
       "        -0.28278336, -0.15559846, -0.30931765, -0.13320339,  0.05399686],\n",
       "       [ 0.38650504, -0.05107119,  0.10648228, -0.01968148,  0.04933858,\n",
       "         0.15601131, -0.4417948 , -0.41216668, -0.18323578, -0.5843336 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqxx6oA73FsH",
    "outputId": "73ab59c8-f765-485b-c66b-a778e212740e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45690113, -0.35882208, -0.21778253,  0.30903298,  0.38628584,\n",
       "        -0.2837154 , -0.15365845, -0.31729227, -0.12548214,  0.04472136],\n",
       "       [ 0.3881424 , -0.04924858,  0.10061955, -0.01443797,  0.04960918,\n",
       "         0.15672725, -0.44318455, -0.40585256, -0.18910766, -0.57734436]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SKnjH7x3Fuv",
    "outputId": "c313ef4c-0fd1-40e3-8ada-ee4f4d40f342"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the learning rate\n",
    "lr = model.optimizer.get_config()['learning_rate']\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FyIdUd-K3dDC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4879487e-01 6.0439014e-01 3.3439300e+00 2.0113649e+00 3.1670466e-02\n",
      "  3.2851154e-01 1.2625482e+00 2.5133371e+00 6.1532621e+00 2.0740637e+01]\n",
      " [4.2184758e-01 3.7008369e+00 5.8266287e+00 3.6317482e+01 5.4547375e-01\n",
      "  4.5680708e-01 3.1358084e-01 1.5557691e+00 3.1050446e+00 1.2105844e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the learning rate to percent change\n",
    "gradient = (old_weights - new_weights)/lr\n",
    "percent_change = abs(100*(old_weights-new_weights)/old_weights)\n",
    "print(percent_change)\n",
    "# Minute changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0_HyVeD3F5d"
   },
   "source": [
    "# To fix this problem, use any of the below methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8t2Hhrp3F8Y"
   },
   "source": [
    "# Method 1: Reduce the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()           # Sequential = Each neuron of one layer connected with each neuron of next layer\n",
    "model2.add(Dense(10, activation = 'sigmoid', input_dim=2))   # Dense Hidden Layer means fully connected. 10 neurons per layer\n",
    "model2.add(Dense(10, activation = 'sigmoid'))                # Adding multiple hidden layers\n",
    "model2.add(Dense(10, activation = 'sigmoid'))\n",
    "model2.add(Dense(1 , activation = 'sigmoid'))                # At final output I need only 1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51395553,  0.29164457, -0.45971018,  0.00895739,  0.44172353,\n",
       "         0.45847005, -0.3118157 ,  0.2577986 ,  0.556806  , -0.4239715 ],\n",
       "       [ 0.37493736, -0.6672539 , -0.46762687,  0.7040755 , -0.34462214,\n",
       "        -0.17195642,  0.38901454, -0.20490533,  0.30279726, -0.24275133]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'] )\n",
    "model2.get_weights()[0]\n",
    "# We get 20 weights because each of the 2 inputs connect to 10 neurons of the 1st hidden layer (10 * 2 = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the above weights for later reference:\n",
    "old_weights2 = model2.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.7100 - accuracy: 0.5100\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.7036 - accuracy: 0.5100\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.7005 - accuracy: 0.5100\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6964 - accuracy: 0.5100\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6955 - accuracy: 0.5100\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6937 - accuracy: 0.5100\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5100\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5100\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.6914 - accuracy: 0.5100\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6911 - accuracy: 0.5100\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.6904 - accuracy: 0.5100\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6901 - accuracy: 0.5100\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6894 - accuracy: 0.5100\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6887 - accuracy: 0.5100\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6880 - accuracy: 0.5100\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6874 - accuracy: 0.7350\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6867 - accuracy: 0.7350\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6862 - accuracy: 0.7450\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6852 - accuracy: 0.7700\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6844 - accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.7500\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6830 - accuracy: 0.7500\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6820 - accuracy: 0.7600\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6809 - accuracy: 0.7600\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6800 - accuracy: 0.7600\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6789 - accuracy: 0.7500\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6780 - accuracy: 0.7550\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6766 - accuracy: 0.7550\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6755 - accuracy: 0.7700\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6740 - accuracy: 0.7600\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6728 - accuracy: 0.7650\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6712 - accuracy: 0.7600\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6696 - accuracy: 0.7650\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6680 - accuracy: 0.7650\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6662 - accuracy: 0.7650\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6644 - accuracy: 0.7700\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6624 - accuracy: 0.7700\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6601 - accuracy: 0.7650\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6580 - accuracy: 0.7550\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6557 - accuracy: 0.7600\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6532 - accuracy: 0.7750\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6506 - accuracy: 0.7750\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.6478 - accuracy: 0.7800\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6446 - accuracy: 0.7650\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6416 - accuracy: 0.7700\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6386 - accuracy: 0.7900\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6350 - accuracy: 0.7850\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6318 - accuracy: 0.7800\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6283 - accuracy: 0.7850\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6245 - accuracy: 0.7850\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6204 - accuracy: 0.7850\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6161 - accuracy: 0.7750\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6120 - accuracy: 0.7900\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.6076 - accuracy: 0.7850\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.6030 - accuracy: 0.7850\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5986 - accuracy: 0.7850\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.5937 - accuracy: 0.7850\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5889 - accuracy: 0.7900\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5841 - accuracy: 0.7900\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5792 - accuracy: 0.7850\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5740 - accuracy: 0.8000\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.5687 - accuracy: 0.8000\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5634 - accuracy: 0.7950\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.8050\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5532 - accuracy: 0.8000\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5476 - accuracy: 0.8000\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5423 - accuracy: 0.8000\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5363 - accuracy: 0.7900\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5306 - accuracy: 0.8000\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5255 - accuracy: 0.8050\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5203 - accuracy: 0.8000\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5151 - accuracy: 0.8050\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5099 - accuracy: 0.8100\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5050 - accuracy: 0.8100\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5002 - accuracy: 0.8100\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4945 - accuracy: 0.8100\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4896 - accuracy: 0.8100\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4848 - accuracy: 0.8100\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.4799 - accuracy: 0.8100\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4754 - accuracy: 0.8100\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4709 - accuracy: 0.8100\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4662 - accuracy: 0.8100\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4613 - accuracy: 0.8100\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 996us/step - loss: 0.4571 - accuracy: 0.8100\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4526 - accuracy: 0.8100\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4483 - accuracy: 0.8100\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.4450 - accuracy: 0.8100\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4407 - accuracy: 0.8100\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4368 - accuracy: 0.8100\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4333 - accuracy: 0.8100\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.4296 - accuracy: 0.8100\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4261 - accuracy: 0.8100\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8100\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.4201 - accuracy: 0.8100\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4163 - accuracy: 0.8150\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.8150\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 0.4100 - accuracy: 0.8150\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4072 - accuracy: 0.8100\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4042 - accuracy: 0.8100\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4014 - accuracy: 0.8100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a60caf8e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, epochs=100)\n",
    "# Note that loss value keeps on reducing up to the last epoch thereby reducing the impact of Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new weights after building the model\n",
    "new_weights2 = model2.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the learning rate\n",
    "lr2 = model2.optimizer.get_config()['learning_rate']\n",
    "lr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 140.09181   120.62496   109.13095  2034.0645    108.78681    89.68023\n",
      "   172.32191   230.79965   103.00352   123.71599 ]\n",
      " [ 281.47568   121.18563   205.91722    22.283812  292.21594   544.03723\n",
      "   264.39062   523.58734   281.75076   415.78995 ]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the learning rate to percent change\n",
    "gradient2 = (old_weights2 - new_weights2)/lr2\n",
    "percent_change2 = abs(100*(old_weights2-new_weights2)/old_weights2)\n",
    "print(percent_change2)\n",
    "# Significant changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Change the Activation Function from 'sigmoid' to 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First test the accuracy with multiple hidden layers\n",
    "\n",
    "model3 = Sequential()           # Sequential = Each neuron of one layer connected with each neuron of next layer\n",
    "model3.add(Dense(10, activation = 'relu', input_dim=2))   # Dense Hidden Layer means fully connected. 10 neurons per layer\n",
    "model3.add(Dense(10, activation = 'relu'))                # Adding multiple hidden layers\n",
    "model3.add(Dense(10, activation = 'relu'))\n",
    "model3.add(Dense(10, activation = 'relu'))\n",
    "model3.add(Dense(10, activation = 'relu'))\n",
    "model3.add(Dense(10, activation = 'relu'))\n",
    "model3.add(Dense(10, activation = 'relu'))\n",
    "model3.add(Dense(10, activation = 'relu'))\n",
    "model3.add(Dense(10, activation = 'relu'))\n",
    "model3.add(Dense(10, activation = 'relu'))\n",
    "model3.add(Dense(10, activation = 'relu'))\n",
    "model3.add(Dense(10, activation = 'relu'))\n",
    "model3.add(Dense(1 , activation = 'sigmoid')) # No need to change it in the last hidden layer as the output will be 0 or 1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3682876 , -0.41917232, -0.36246863, -0.51950645, -0.27421314,\n",
       "        -0.4615324 , -0.41363978,  0.15740973, -0.402262  ,  0.39492005],\n",
       "       [-0.13176167,  0.69669265, -0.3866923 , -0.61544883,  0.14110816,\n",
       "         0.32514924,  0.06273746, -0.51373386,  0.63674647,  0.02457398]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'] )\n",
    "model3.get_weights()[0]\n",
    "# We get 20 weights because each of the 2 inputs connect to 10 neurons of the 1st hidden layer (10 * 2 = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the above weights for later reference:\n",
    "old_weights3 = model3.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 1s 1ms/step - loss: 0.6934 - accuracy: 0.4300\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5350\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6921 - accuracy: 0.7900\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.6600\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.6900\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6816 - accuracy: 0.7650\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6747 - accuracy: 0.7800\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6645 - accuracy: 0.7950\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6495 - accuracy: 0.8150\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6278 - accuracy: 0.7950\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6024 - accuracy: 0.8350\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5755 - accuracy: 0.8750\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5439 - accuracy: 0.8600\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5181 - accuracy: 0.8550\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.8750\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4973 - accuracy: 0.8750\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.8900\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4638 - accuracy: 0.9000\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4304 - accuracy: 0.9150\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4073 - accuracy: 0.9250\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.9300\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3608 - accuracy: 0.9350\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.3506 - accuracy: 0.9300\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.9550\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2935 - accuracy: 0.9550\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.9600\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9700\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9700\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9700\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9750\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9850\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0700 - accuracy: 0.9850\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0642 - accuracy: 0.9750\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 0.9850\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0377 - accuracy: 0.9900\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0372 - accuracy: 0.9950\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0267 - accuracy: 0.9950\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.8073e-04 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.3870e-04 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 8.8710e-04 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 8.3729e-04 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.9895e-04 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 7.6534e-04 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 7.2667e-04 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 6.9874e-04 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7332e-04 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 6.4161e-04 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 6.1569e-04 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 5.8845e-04 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 5.6035e-04 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 5.4591e-04 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 5.2834e-04 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 5.0310e-04 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.6679e-04 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 4.1094e-04 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 4.0005e-04 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 3.8177e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5508e-04 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 3.3459e-04 - accuracy: 1.0000\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 3.1132e-04 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 2.9508e-04 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 2.8381e-04 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.7162e-04 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 2.5892e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 2.4432e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3223e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2184e-04 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 2.1458e-04 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 2.0794e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.9487e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.8794e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.8354e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.7412e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.7330e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.6530e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.5642e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.4685e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a60cbea60>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(x_train, y_train, epochs=100)\n",
    "# Note that loss value is minimised and accuracy is 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new weights after building the model\n",
    "new_weights3 = model3.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the learning rate\n",
    "lr3 = model3.optimizer.get_config()['learning_rate']\n",
    "lr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 35.01251    22.147198    4.9264584  18.899559   16.977608    6.7661953\n",
      "   21.5092     75.86525    44.09538    54.28038  ]\n",
      " [120.46381    13.535548    1.4499154  15.065355   54.997704    6.2474847\n",
      "   27.17787    17.358683   18.039877  589.06647  ]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the learning rate to percent change\n",
    "gradient3 = (old_weights3 - new_weights3)/lr3\n",
    "percent_change3 = abs(100*(old_weights3-new_weights3)/old_weights3)\n",
    "print(percent_change3)\n",
    "# Significant changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hence Vanishing Gradient Problem can be minimised by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Reducing the number of hidden layers \n",
    "\n",
    "# 2) Changing the activation function to RELU"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
